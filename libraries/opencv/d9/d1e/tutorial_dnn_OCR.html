<!-- HTML header for doxygen 1.8.6-->
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=9"/>
<meta name="generator" content="Doxygen 1.9.8"/>
<title>OpenCV: How to run custom OCR model</title>
<link href="../../opencv.ico" rel="shortcut icon" type="image/x-icon" />
<link href="../../tabs.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="../../jquery.js"></script>
<script type="text/javascript" src="../../dynsections.js"></script>
<script type="text/javascript" src="../../tutorial-utils.js"></script>
<link href="../../search/search.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="../../search/searchdata.js"></script>
<script type="text/javascript" src="../../search/search.js"></script>
<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script type="text/javascript">
window.MathJax = {
  options: {
    ignoreHtmlClass: 'tex2jax_ignore',
    processHtmlClass: 'tex2jax_process'
  },
  loader: {
    load: ['[tex]/ams']
  },
  tex: {
    macros: {},
    packages: ['base','configmacros','ams']
  }
};
//<![CDATA[
window.MathJax = {
    loader: {load: ['[tex]/ams']},
    tex: {
        packages: {'[+]': ['ams']},
        macros: {
            matTT: [ "\\[ \\left|\\begin{array}{ccc} #1 & #2 & #3\\\\ #4 & #5 & #6\\\\ #7 & #8 & #9 \\end{array}\\right| \\]", 9],
            fork: ["\\left\\{ \\begin{array}{l l} #1 & \\mbox{#2}\\\\ #3 & \\mbox{#4}\\\\ \\end{array} \\right.", 4],
            forkthree: ["\\left\\{ \\begin{array}{l l} #1 & \\mbox{#2}\\\\ #3 & \\mbox{#4}\\\\ #5 & \\mbox{#6}\\\\ \\end{array} \\right.", 6],
            forkfour: ["\\left\\{ \\begin{array}{l l} #1 & \\mbox{#2}\\\\ #3 & \\mbox{#4}\\\\ #5 & \\mbox{#6}\\\\ #7 & \\mbox{#8}\\\\ \\end{array} \\right.", 8],
            vecthree: ["\\begin{bmatrix} #1\\\\ #2\\\\ #3 \\end{bmatrix}", 3],
            vecthreethree: ["\\begin{bmatrix} #1 & #2 & #3\\\\ #4 & #5 & #6\\\\ #7 & #8 & #9 \\end{bmatrix}", 9],
            cameramatrix: ["#1 = \\begin{bmatrix} f_x & 0 & c_x\\\\ 0 & f_y & c_y\\\\ 0 & 0 & 1 \\end{bmatrix}", 1],
            distcoeffs: ["(k_1, k_2, p_1, p_2[, k_3[, k_4, k_5, k_6 [, s_1, s_2, s_3, s_4[, \\tau_x, \\tau_y]]]]) \\text{ of 4, 5, 8, 12 or 14 elements}"],
            distcoeffsfisheye: ["(k_1, k_2, k_3, k_4)"],
            hdotsfor: ["\\dots", 1],
            mathbbm: ["\\mathbb{#1}", 1],
            bordermatrix: ["\\matrix{#1}", 1]
        },
        processEscapes: false
    }
};
//]]>
</script>
<script type="text/javascript" id="MathJax-script" async="async" src="https://cdn.jsdelivr.net/npm/mathjax@3.0.1/es5/tex-chtml.js"></script>
<link href="../../doxygen.css" rel="stylesheet" type="text/css" />
<link href="../../stylesheet.css" rel="stylesheet" type="text/css"/>
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
<!--#include virtual="/google-search.html"-->
<table cellspacing="0" cellpadding="0">
 <tbody>
 <tr style="height: 56px;">
  <td id="projectlogo"><img alt="Logo" src="../../opencv-logo-small.png"/></td>
  <td style="padding-left: 0.5em;">
   <div id="projectname">OpenCV
   &#160;<span id="projectnumber">4.10.0</span>
   </div>
   <div id="projectbrief">Open Source Computer Vision</div>
  </td>
 </tr>
 </tbody>
</table>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.9.8 -->
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:d3d9a9a6595521f9666a5e94cc830dab83b65699&amp;dn=expat.txt MIT */
var searchBox = new SearchBox("searchBox", "../../search/",'.html');
/* @license-end */
</script>
<script type="text/javascript" src="../../menudata.js"></script>
<script type="text/javascript" src="../../menu.js"></script>
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:d3d9a9a6595521f9666a5e94cc830dab83b65699&amp;dn=expat.txt MIT */
$(function() {
  initMenu('../../',true,false,'search.php','Search');
  $(document).ready(function() { init_search(); });
});
/* @license-end */
</script>
<div id="main-nav"></div>
<!-- window showing the filter options -->
<div id="MSearchSelectWindow"
     onmouseover="return searchBox.OnSearchSelectShow()"
     onmouseout="return searchBox.OnSearchSelectHide()"
     onkeydown="return searchBox.OnSearchSelectKey(event)">
</div>

<!-- iframe showing the search results (closed by default) -->
<div id="MSearchResultsWindow">
<div id="MSearchResults">
<div class="SRPage">
<div id="SRIndex">
<div id="SRResults"></div>
<div class="SRStatus" id="Loading">Loading...</div>
<div class="SRStatus" id="Searching">Searching...</div>
<div class="SRStatus" id="NoMatches">No Matches</div>
</div>
</div>
</div>
</div>

<div id="nav-path" class="navpath">
  <ul>
<li class="navelem"><a class="el" href="../../d9/df8/tutorial_root.html">OpenCV Tutorials</a></li><li class="navelem"><a class="el" href="../../d2/d58/tutorial_table_of_content_dnn.html">Deep Neural Networks (dnn module)</a></li>  </ul>
</div>
</div><!-- top -->
<div><div class="header">
  <div class="headertitle"><div class="title">How to run custom OCR model</div></div>
</div><!--header-->
<div class="contents">
<div class="toc"><h3>Table of Contents</h3>
<ul><li class="level1"><a href="#autotoc_md379">Introduction</a></li>
<li class="level1"><a href="#autotoc_md380">Train your own OCR model</a></li>
<li class="level1"><a href="#autotoc_md381">Transform OCR model to ONNX format and Use it in OpenCV DNN</a><ul><li class="level2"><a href="#autotoc_md382">Execute in webcam</a></li>
</ul>
</li>
<li class="level1"><a href="#autotoc_md383">Pre-trained ONNX models are provided</a><ul><li class="level2"><a href="#autotoc_md384">Model selection suggestion</a></li>
</ul>
</li>
</ul>
</div>
<div class="textblock"><p><b>Prev Tutorial:</b> <a class="el" href="../../dc/db1/tutorial_dnn_custom_layers.html">Custom deep learning layers support</a> <br  />
<b>Next Tutorial:</b> <a class="el" href="../../d4/d43/tutorial_dnn_text_spotting.html">High Level API: TextDetectionModel and TextRecognitionModel</a> <br  />
 </p><table class="markdownTable">
<tr class="markdownTableHead">
<th class="markdownTableHeadRight"></th><th class="markdownTableHeadLeft"></th></tr>
<tr class="markdownTableRowOdd">
<td class="markdownTableBodyRight">Original author   </td><td class="markdownTableBodyLeft">Zihao Mu    </td></tr>
<tr class="markdownTableRowEven">
<td class="markdownTableBodyRight">Compatibility   </td><td class="markdownTableBodyLeft">OpenCV &gt;= 4.3   </td></tr>
</table>
<h1><a class="anchor" id="autotoc_md379"></a>
Introduction</h1>
<p>In this tutorial, we first introduce how to obtain the custom OCR model, then how to transform your own OCR models so that they can be run correctly by the opencv_dnn module. and finally we will provide some pre-trained models.</p>
<h1><a class="anchor" id="autotoc_md380"></a>
Train your own OCR model</h1>
<p><a href="https://github.com/zihaomu/deep-text-recognition-benchmark" target="_blank">This repository</a> is a good start point for training your own OCR model. In repository, the MJSynth+SynthText was set as training set by default. In addition, you can configure the model structure and data set you want.</p>
<h1><a class="anchor" id="autotoc_md381"></a>
Transform OCR model to ONNX format and Use it in OpenCV DNN</h1>
<p>After completing the model training, please use <a href="https://github.com/zihaomu/deep-text-recognition-benchmark/blob/master/transform_to_onnx.py" target="_blank">transform_to_onnx.py</a> to convert the model into onnx format.</p>
<h2><a class="anchor" id="autotoc_md382"></a>
Execute in webcam</h2>
<p>The Python version example code can be found at <a href="https://github.com/opencv/opencv/blob/4.x/samples/dnn/text_detection.py" target="_blank">here</a>.</p>
<p>Example: </p><div class="fragment"><div class="line">$ text_detection -m=[path_to_text_detect_model] -ocr=[path_to_text_recognition_model]</div>
</div><!-- fragment --><h1><a class="anchor" id="autotoc_md383"></a>
Pre-trained ONNX models are provided</h1>
<p>Some pre-trained models can be found at <a href="https://drive.google.com/drive/folders/1cTbQ3nuZG-EKWak6emD_s8_hHXWz7lAr?usp=sharing">https://drive.google.com/drive/folders/1cTbQ3nuZG-EKWak6emD_s8_hHXWz7lAr?usp=sharing</a>.</p>
<p>Their performance at different text recognition datasets is shown in the table below:</p>
<table class="markdownTable">
<tr class="markdownTableHead">
<th class="markdownTableHeadNone">Model name   </th><th class="markdownTableHeadNone">IIIT5k(%)   </th><th class="markdownTableHeadNone">SVT(%)   </th><th class="markdownTableHeadNone">ICDAR03(%)   </th><th class="markdownTableHeadNone">ICDAR13(%)   </th><th class="markdownTableHeadNone">ICDAR15(%)   </th><th class="markdownTableHeadNone">SVTP(%)   </th><th class="markdownTableHeadNone">CUTE80(%)   </th><th class="markdownTableHeadNone">average acc (%)   </th><th class="markdownTableHeadNone">parameter( x10^6 )    </th></tr>
<tr class="markdownTableRowOdd">
<td class="markdownTableBodyNone">DenseNet-CTC   </td><td class="markdownTableBodyNone">72.267   </td><td class="markdownTableBodyNone">67.39   </td><td class="markdownTableBodyNone">82.81   </td><td class="markdownTableBodyNone">80   </td><td class="markdownTableBodyNone">48.38   </td><td class="markdownTableBodyNone">49.45   </td><td class="markdownTableBodyNone">42.50   </td><td class="markdownTableBodyNone">63.26   </td><td class="markdownTableBodyNone">0.24    </td></tr>
<tr class="markdownTableRowEven">
<td class="markdownTableBodyNone">DenseNet-BiLSTM-CTC   </td><td class="markdownTableBodyNone">73.76   </td><td class="markdownTableBodyNone">72.33   </td><td class="markdownTableBodyNone">86.15   </td><td class="markdownTableBodyNone">83.15   </td><td class="markdownTableBodyNone">50.67   </td><td class="markdownTableBodyNone">57.984   </td><td class="markdownTableBodyNone">49.826   </td><td class="markdownTableBodyNone">67.69   </td><td class="markdownTableBodyNone">3.63    </td></tr>
<tr class="markdownTableRowOdd">
<td class="markdownTableBodyNone">VGG-CTC   </td><td class="markdownTableBodyNone">75.96   </td><td class="markdownTableBodyNone">75.42   </td><td class="markdownTableBodyNone">85.92   </td><td class="markdownTableBodyNone">83.54   </td><td class="markdownTableBodyNone">54.89   </td><td class="markdownTableBodyNone">57.52   </td><td class="markdownTableBodyNone">50.17   </td><td class="markdownTableBodyNone">69.06   </td><td class="markdownTableBodyNone">5.57    </td></tr>
<tr class="markdownTableRowEven">
<td class="markdownTableBodyNone">CRNN_VGG-BiLSTM-CTC   </td><td class="markdownTableBodyNone">82.63   </td><td class="markdownTableBodyNone">82.07   </td><td class="markdownTableBodyNone">92.96   </td><td class="markdownTableBodyNone">88.867   </td><td class="markdownTableBodyNone">66.28   </td><td class="markdownTableBodyNone">71.01   </td><td class="markdownTableBodyNone">62.37   </td><td class="markdownTableBodyNone">78.03   </td><td class="markdownTableBodyNone">8.45    </td></tr>
<tr class="markdownTableRowOdd">
<td class="markdownTableBodyNone">ResNet-CTC   </td><td class="markdownTableBodyNone">84.00   </td><td class="markdownTableBodyNone">84.08   </td><td class="markdownTableBodyNone">92.39   </td><td class="markdownTableBodyNone">88.96   </td><td class="markdownTableBodyNone">67.74   </td><td class="markdownTableBodyNone">74.73   </td><td class="markdownTableBodyNone">67.60   </td><td class="markdownTableBodyNone">79.93   </td><td class="markdownTableBodyNone">44.28   </td></tr>
</table>
<p>The performance of the text recognition model were tested on OpenCV DNN, and does not include the text detection model.</p>
<h2><a class="anchor" id="autotoc_md384"></a>
Model selection suggestion</h2>
<p>The input of text recognition model is the output of the text detection model, which causes the performance of text detection to greatly affect the performance of text recognition.</p>
<p>DenseNet_CTC has the smallest parameters and best FPS, and it is suitable for edge devices, which are very sensitive to the cost of calculation. If you have limited computing resources and want to achieve better accuracy, VGG_CTC is a good choice.</p>
<p>CRNN_VGG_BiLSTM_CTC is suitable for scenarios that require high recognition accuracy. </p>
</div></div><!-- contents -->
</div><!-- PageDoc -->
<!-- HTML footer for doxygen 1.8.6-->
<!-- start footer part -->
<hr class="footer"/><address class="footer"><small>
Generated on Sun Jun 2 2024 21:52:13 for OpenCV by &#160;<a href="http://www.doxygen.org/index.html">
<img class="footer" src="../../doxygen.png" alt="doxygen"/>
</a> 1.9.8
</small></address>
<script type="text/javascript">
//<![CDATA[
addTutorialsButtons();
//]]>
</script>
</body>
</html>
