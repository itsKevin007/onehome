<!-- HTML header for doxygen 1.8.6-->
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=9"/>
<meta name="generator" content="Doxygen 1.9.8"/>
<title>OpenCV: Camera calibration With OpenCV</title>
<link href="../../opencv.ico" rel="shortcut icon" type="image/x-icon" />
<link href="../../tabs.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="../../jquery.js"></script>
<script type="text/javascript" src="../../dynsections.js"></script>
<script type="text/javascript" src="../../tutorial-utils.js"></script>
<link href="../../search/search.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="../../search/searchdata.js"></script>
<script type="text/javascript" src="../../search/search.js"></script>
<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script type="text/javascript">
window.MathJax = {
  options: {
    ignoreHtmlClass: 'tex2jax_ignore',
    processHtmlClass: 'tex2jax_process'
  },
  loader: {
    load: ['[tex]/ams']
  },
  tex: {
    macros: {},
    packages: ['base','configmacros','ams']
  }
};
//<![CDATA[
window.MathJax = {
    loader: {load: ['[tex]/ams']},
    tex: {
        packages: {'[+]': ['ams']},
        macros: {
            matTT: [ "\\[ \\left|\\begin{array}{ccc} #1 & #2 & #3\\\\ #4 & #5 & #6\\\\ #7 & #8 & #9 \\end{array}\\right| \\]", 9],
            fork: ["\\left\\{ \\begin{array}{l l} #1 & \\mbox{#2}\\\\ #3 & \\mbox{#4}\\\\ \\end{array} \\right.", 4],
            forkthree: ["\\left\\{ \\begin{array}{l l} #1 & \\mbox{#2}\\\\ #3 & \\mbox{#4}\\\\ #5 & \\mbox{#6}\\\\ \\end{array} \\right.", 6],
            forkfour: ["\\left\\{ \\begin{array}{l l} #1 & \\mbox{#2}\\\\ #3 & \\mbox{#4}\\\\ #5 & \\mbox{#6}\\\\ #7 & \\mbox{#8}\\\\ \\end{array} \\right.", 8],
            vecthree: ["\\begin{bmatrix} #1\\\\ #2\\\\ #3 \\end{bmatrix}", 3],
            vecthreethree: ["\\begin{bmatrix} #1 & #2 & #3\\\\ #4 & #5 & #6\\\\ #7 & #8 & #9 \\end{bmatrix}", 9],
            cameramatrix: ["#1 = \\begin{bmatrix} f_x & 0 & c_x\\\\ 0 & f_y & c_y\\\\ 0 & 0 & 1 \\end{bmatrix}", 1],
            distcoeffs: ["(k_1, k_2, p_1, p_2[, k_3[, k_4, k_5, k_6 [, s_1, s_2, s_3, s_4[, \\tau_x, \\tau_y]]]]) \\text{ of 4, 5, 8, 12 or 14 elements}"],
            distcoeffsfisheye: ["(k_1, k_2, k_3, k_4)"],
            hdotsfor: ["\\dots", 1],
            mathbbm: ["\\mathbb{#1}", 1],
            bordermatrix: ["\\matrix{#1}", 1]
        },
        processEscapes: false
    }
};
//]]>
</script>
<script type="text/javascript" id="MathJax-script" async="async" src="https://cdn.jsdelivr.net/npm/mathjax@3.0.1/es5/tex-chtml.js"></script>
<link href="../../doxygen.css" rel="stylesheet" type="text/css" />
<link href="../../stylesheet.css" rel="stylesheet" type="text/css"/>
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
<!--#include virtual="/google-search.html"-->
<table cellspacing="0" cellpadding="0">
 <tbody>
 <tr style="height: 56px;">
  <td id="projectlogo"><img alt="Logo" src="../../opencv-logo-small.png"/></td>
  <td style="padding-left: 0.5em;">
   <div id="projectname">OpenCV
   &#160;<span id="projectnumber">4.10.0</span>
   </div>
   <div id="projectbrief">Open Source Computer Vision</div>
  </td>
 </tr>
 </tbody>
</table>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.9.8 -->
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:d3d9a9a6595521f9666a5e94cc830dab83b65699&amp;dn=expat.txt MIT */
var searchBox = new SearchBox("searchBox", "../../search/",'.html');
/* @license-end */
</script>
<script type="text/javascript" src="../../menudata.js"></script>
<script type="text/javascript" src="../../menu.js"></script>
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:d3d9a9a6595521f9666a5e94cc830dab83b65699&amp;dn=expat.txt MIT */
$(function() {
  initMenu('../../',true,false,'search.php','Search');
  $(document).ready(function() { init_search(); });
});
/* @license-end */
</script>
<div id="main-nav"></div>
<!-- window showing the filter options -->
<div id="MSearchSelectWindow"
     onmouseover="return searchBox.OnSearchSelectShow()"
     onmouseout="return searchBox.OnSearchSelectHide()"
     onkeydown="return searchBox.OnSearchSelectKey(event)">
</div>

<!-- iframe showing the search results (closed by default) -->
<div id="MSearchResultsWindow">
<div id="MSearchResults">
<div class="SRPage">
<div id="SRIndex">
<div id="SRResults"></div>
<div class="SRStatus" id="Loading">Loading...</div>
<div class="SRStatus" id="Searching">Searching...</div>
<div class="SRStatus" id="NoMatches">No Matches</div>
</div>
</div>
</div>
</div>

<div id="nav-path" class="navpath">
  <ul>
<li class="navelem"><a class="el" href="../../d9/df8/tutorial_root.html">OpenCV Tutorials</a></li><li class="navelem"><a class="el" href="../../d6/d55/tutorial_table_of_content_calib3d.html">Camera calibration and 3D reconstruction (calib3d module)</a></li>  </ul>
</div>
</div><!-- top -->
<div><div class="header">
  <div class="headertitle"><div class="title">Camera calibration With OpenCV</div></div>
</div><!--header-->
<div class="contents">
<div class="toc"><h3>Table of Contents</h3>
<ul><li class="level1"><a href="#autotoc_md229">Theory</a></li>
<li class="level1"><a href="#autotoc_md230">Goal</a></li>
<li class="level1"><a href="#autotoc_md231">Source code</a></li>
<li class="level1"><a href="#autotoc_md232">Explanation</a></li>
<li class="level1"><a href="#autotoc_md233">The calibration and save</a></li>
<li class="level1"><a href="#autotoc_md234">Results</a></li>
</ul>
</div>
<div class="textblock"><p><b>Prev Tutorial:</b> <a class="el" href="../../dc/d43/tutorial_camera_calibration_square_chess.html">Camera calibration with square chessboard</a> <br  />
<b>Next Tutorial:</b> <a class="el" href="../../dc/d2c/tutorial_real_time_pose.html">Real Time pose estimation of a textured object</a> <br  />
 </p><table class="markdownTable">
<tr class="markdownTableHead">
<th class="markdownTableHeadRight"></th><th class="markdownTableHeadLeft"></th></tr>
<tr class="markdownTableRowOdd">
<td class="markdownTableBodyRight">Original author   </td><td class="markdownTableBodyLeft">Bernát Gábor    </td></tr>
<tr class="markdownTableRowEven">
<td class="markdownTableBodyRight">Compatibility   </td><td class="markdownTableBodyLeft">OpenCV &gt;= 4.0   </td></tr>
</table>
<p>Cameras have been around for a long-long time. However, with the introduction of the cheap <em>pinhole</em> cameras in the late 20th century, they became a common occurrence in our everyday life. Unfortunately, this cheapness comes with its price: significant distortion. Luckily, these are constants and with a calibration and some remapping we can correct this. Furthermore, with calibration you may also determine the relation between the camera's natural units (pixels) and the real world units (for example millimeters).</p>
<h1><a class="anchor" id="autotoc_md229"></a>
Theory</h1>
<p>For the distortion OpenCV takes into account the radial and tangential factors. For the radial factor one uses the following formula:</p>
<p class="formulaDsp">
\[x_{distorted} = x( 1 + k_1 r^2 + k_2 r^4 + k_3 r^6) \\
y_{distorted} = y( 1 + k_1 r^2 + k_2 r^4 + k_3 r^6)\]
</p>
<p>So for an undistorted pixel point at \((x,y)\) coordinates, its position on the distorted image will be \((x_{distorted} y_{distorted})\). The presence of the radial distortion manifests in form of the "barrel" or "fish-eye" effect.</p>
<p>Tangential distortion occurs because the image taking lenses are not perfectly parallel to the imaging plane. It can be represented via the formulas:</p>
<p class="formulaDsp">
\[x_{distorted} = x + [ 2p_1xy + p_2(r^2+2x^2)] \\
y_{distorted} = y + [ p_1(r^2+ 2y^2)+ 2p_2xy]\]
</p>
<p>So we have five distortion parameters which in OpenCV are presented as one row matrix with 5 columns:</p>
<p class="formulaDsp">
\[distortion\_coefficients=(k_1 \hspace{10pt} k_2 \hspace{10pt} p_1 \hspace{10pt} p_2 \hspace{10pt} k_3)\]
</p>
<p>Now for the unit conversion we use the following formula:</p>
<p class="formulaDsp">
\[\left [  \begin{matrix}   x \\   y \\  w \end{matrix} \right ] = \left [ \begin{matrix}   f_x &amp; 0 &amp; c_x \\  0 &amp; f_y &amp; c_y \\   0 &amp; 0 &amp; 1 \end{matrix} \right ] \left [ \begin{matrix}  X \\  Y \\   Z \end{matrix} \right ]\]
</p>
<p>Here the presence of \(w\) is explained by the use of homography coordinate system (and \(w=Z\)). The unknown parameters are \(f_x\) and \(f_y\) (camera focal lengths) and \((c_x, c_y)\) which are the optical centers expressed in pixels coordinates. If for both axes a common focal length is used with a given \(a\) aspect ratio (usually 1), then \(f_y=f_x*a\) and in the upper formula we will have a single focal length \(f\). The matrix containing these four parameters is referred to as the <em>camera matrix</em>. While the distortion coefficients are the same regardless of the camera resolutions used, these should be scaled along with the current resolution from the calibrated resolution.</p>
<p>The process of determining these two matrices is the calibration. Calculation of these parameters is done through basic geometrical equations. The equations used depend on the chosen calibrating objects. Currently OpenCV supports three types of objects for calibration:</p>
<ul>
<li>Classical black-white chessboard</li>
<li>ChArUco board pattern</li>
<li>Symmetrical circle pattern</li>
<li>Asymmetrical circle pattern</li>
</ul>
<p>Basically, you need to take snapshots of these patterns with your camera and let OpenCV find them. Each found pattern results in a new equation. To solve the equation you need at least a predetermined number of pattern snapshots to form a well-posed equation system. This number is higher for the chessboard pattern and less for the circle ones. For example, in theory the chessboard pattern requires at least two snapshots. However, in practice we have a good amount of noise present in our input images, so for good results you will probably need at least 10 good snapshots of the input pattern in different positions.</p>
<h1><a class="anchor" id="autotoc_md230"></a>
Goal</h1>
<p>The sample application will:</p>
<ul>
<li>Determine the distortion matrix</li>
<li>Determine the camera matrix</li>
<li>Take input from Camera, Video and Image file list</li>
<li>Read configuration from XML/YAML file</li>
<li>Save the results into XML/YAML file</li>
<li>Calculate re-projection error</li>
</ul>
<h1><a class="anchor" id="autotoc_md231"></a>
Source code</h1>
<p>You may also find the source code in the <code>samples/cpp/tutorial_code/calib3d/camera_calibration/</code> folder of the OpenCV source library or <a href="https://github.com/opencv/opencv/tree/4.x/samples/cpp/tutorial_code/calib3d/camera_calibration/camera_calibration.cpp" target="_blank">download it from here</a>. For the usage of the program, run it with <code>-h</code> argument. The program has an essential argument: the name of its configuration file. If none is given then it will try to open the one named "default.xml". <a href="https://github.com/opencv/opencv/tree/4.x/samples/cpp/tutorial_code/calib3d/camera_calibration/in_VID5.xml" target="_blank">Here's a sample configuration file</a> in XML format. In the configuration file you may choose to use camera as an input, a video file or an image list. If you opt for the last one, you will need to create a configuration file where you enumerate the images to use. Here's <a href="https://github.com/opencv/opencv/tree/4.x/samples/cpp/tutorial_code/calib3d/camera_calibration/VID5.xml" target="_blank">an example of this</a>. The important part to remember is that the images need to be specified using the absolute path or the relative one from your application's working directory. You may find all this in the samples directory mentioned above.</p>
<p>The application starts up with reading the settings from the configuration file. Although, this is an important part of it, it has nothing to do with the subject of this tutorial: <em>camera calibration</em>. Therefore, I've chosen not to post the code for that part here. Technical background on how to do this you can find in the <a class="el" href="../../dd/d74/tutorial_file_input_output_with_xml_yml.html">File Input and Output using XML and YAML files</a> tutorial.</p>
<h1><a class="anchor" id="autotoc_md232"></a>
Explanation</h1>
<ol type="1">
<li><p class="startli"><b>Read the settings</b> </p><div class="fragment"><div class="line">    Settings s;</div>
<div class="line">    <span class="keyword">const</span> <span class="keywordtype">string</span> inputSettingsFile = parser.get&lt;<span class="keywordtype">string</span>&gt;(0);</div>
<div class="line">    FileStorage fs(inputSettingsFile, FileStorage::READ); <span class="comment">// Read the settings</span></div>
<div class="line">    <span class="keywordflow">if</span> (!fs.isOpened())</div>
<div class="line">    {</div>
<div class="line">        cout &lt;&lt; <span class="stringliteral">&quot;Could not open the configuration file: \&quot;&quot;</span> &lt;&lt; inputSettingsFile &lt;&lt; <span class="stringliteral">&quot;\&quot;&quot;</span> &lt;&lt; endl;</div>
<div class="line">        parser.printMessage();</div>
<div class="line">        <span class="keywordflow">return</span> -1;</div>
<div class="line">    }</div>
<div class="line">    fs[<span class="stringliteral">&quot;Settings&quot;</span>] &gt;&gt; s;</div>
<div class="line">    fs.release();                                         <span class="comment">// close Settings file</span></div>
</div><!-- fragment --><p class="startli">For this I've used simple OpenCV class input operation. After reading the file I've an additional post-processing function that checks validity of the input. Only if all inputs are good then <em>goodInput</em> variable will be true.</p>
</li>
<li><p class="startli"><b>Get next input, if it fails or we have enough of them - calibrate</b></p>
<p class="startli">After this we have a big loop where we do the following operations: get the next image from the image list, camera or video file. If this fails or we have enough images then we run the calibration process. In case of image we step out of the loop and otherwise the remaining frames will be undistorted (if the option is set) via changing from <em>DETECTION</em> mode to the <em>CALIBRATED</em> one. </p><div class="fragment"><div class="line"> </div>
<div class="line">    <span class="keywordflow">for</span>(;;)</div>
<div class="line">    {</div>
<div class="line">        Mat view;</div>
<div class="line">        <span class="keywordtype">bool</span> blinkOutput = <span class="keyword">false</span>;</div>
<div class="line"> </div>
<div class="line">        view = s.nextImage();</div>
<div class="line"> </div>
<div class="line">        <span class="comment">//-----  If no more image, or got enough, then stop calibration and show result -------------</span></div>
<div class="line">        <span class="keywordflow">if</span>( mode == CAPTURING &amp;&amp; imagePoints.size() &gt;= (<span class="keywordtype">size_t</span>)s.nrFrames )</div>
<div class="line">        {</div>
<div class="line">          <span class="keywordflow">if</span>(runCalibrationAndSave(s, imageSize,  cameraMatrix, distCoeffs, imagePoints, grid_width,</div>
<div class="line">                                   release_object))</div>
<div class="line">              mode = CALIBRATED;</div>
<div class="line">          <span class="keywordflow">else</span></div>
<div class="line">              mode = DETECTION;</div>
<div class="line">        }</div>
<div class="line">        <span class="keywordflow">if</span>(view.empty())          <span class="comment">// If there are no more images stop the loop</span></div>
<div class="line">        {</div>
<div class="line">            <span class="comment">// if calibration threshold was not reached yet, calibrate now</span></div>
<div class="line">            <span class="keywordflow">if</span>( mode != CALIBRATED &amp;&amp; !imagePoints.empty() )</div>
<div class="line">                runCalibrationAndSave(s, imageSize,  cameraMatrix, distCoeffs, imagePoints, grid_width,</div>
<div class="line">                                      release_object);</div>
<div class="line">            <span class="keywordflow">break</span>;</div>
<div class="line">        }</div>
</div><!-- fragment --><p> For some cameras we may need to flip the input image. Here we do this too.</p>
</li>
<li><p class="startli"><b>Find the pattern in the current input</b></p>
<p class="startli">The formation of the equations I mentioned above aims to finding major patterns in the input: in case of the chessboard this are corners of the squares and for the circles, well, the circles themselves. ChArUco board is equivalent to chessboard, but corners are mached by ArUco markers. The position of these will form the result which will be written into the <em>pointBuf</em> vector. </p><div class="fragment"><div class="line">        vector&lt;Point2f&gt; pointBuf;</div>
<div class="line"> </div>
<div class="line">        <span class="keywordtype">bool</span> found;</div>
<div class="line"> </div>
<div class="line">        <span class="keywordtype">int</span> chessBoardFlags = CALIB_CB_ADAPTIVE_THRESH | CALIB_CB_NORMALIZE_IMAGE;</div>
<div class="line"> </div>
<div class="line">        <span class="keywordflow">if</span>(!s.useFisheye) {</div>
<div class="line">            <span class="comment">// fast check erroneously fails with high distortions like fisheye</span></div>
<div class="line">            chessBoardFlags |= CALIB_CB_FAST_CHECK;</div>
<div class="line">        }</div>
<div class="line"> </div>
<div class="line">        <span class="keywordflow">switch</span>( s.calibrationPattern ) <span class="comment">// Find feature points on the input format</span></div>
<div class="line">        {</div>
<div class="line">        <span class="keywordflow">case</span> Settings::CHESSBOARD:</div>
<div class="line">            found = findChessboardCorners( view, s.boardSize, pointBuf, chessBoardFlags);</div>
<div class="line">            <span class="keywordflow">break</span>;</div>
<div class="line">        <span class="keywordflow">case</span> Settings::CHARUCOBOARD:</div>
<div class="line">            ch_detector.detectBoard( view, pointBuf, markerIds);</div>
<div class="line">            found = pointBuf.size() == (size_t)((s.boardSize.height - 1)*(s.boardSize.width - 1));</div>
<div class="line">            <span class="keywordflow">break</span>;</div>
<div class="line">        <span class="keywordflow">case</span> Settings::CIRCLES_GRID:</div>
<div class="line">            found = findCirclesGrid( view, s.boardSize, pointBuf );</div>
<div class="line">            <span class="keywordflow">break</span>;</div>
<div class="line">        <span class="keywordflow">case</span> Settings::ASYMMETRIC_CIRCLES_GRID:</div>
<div class="line">            found = findCirclesGrid( view, s.boardSize, pointBuf, CALIB_CB_ASYMMETRIC_GRID );</div>
<div class="line">            <span class="keywordflow">break</span>;</div>
<div class="line">        <span class="keywordflow">default</span>:</div>
<div class="line">            found = <span class="keyword">false</span>;</div>
<div class="line">            <span class="keywordflow">break</span>;</div>
<div class="line">        }</div>
</div><!-- fragment --><p> Depending on the type of the input pattern you use either the <a class="el" href="../../d9/d0c/group__calib3d.html#ga93efa9b0aa890de240ca32b11253dd4a">cv::findChessboardCorners</a> or the <a class="el" href="../../d9/d0c/group__calib3d.html#ga7f02cd21c8352142890190227628fa80">cv::findCirclesGrid</a> function or <a class="el" href="../../d9/df5/classcv_1_1aruco_1_1CharucoDetector.html#aacbea601612a3a0feaa45ebb7fb255fd">cv::aruco::CharucoDetector::detectBoard</a> method. For all of them you pass the current image and the size of the board and you'll get the positions of the patterns. <a class="el" href="../../d9/d0c/group__calib3d.html#ga93efa9b0aa890de240ca32b11253dd4a" title="Finds the positions of internal corners of the chessboard.">cv::findChessboardCorners</a> and <a class="el" href="../../d9/d0c/group__calib3d.html#ga7f02cd21c8352142890190227628fa80" title="Finds centers in the grid of circles.">cv::findCirclesGrid</a> return a boolean variable which states if the pattern was found in the input (we only need to take into account those images where this is true!). <code>CharucoDetector::detectBoard</code> may detect partially visible pattern and returns coordunates and ids of visible inner corners.</p>
<dl class="section note"><dt>Note</dt><dd>Board size and amount of matched points is different for chessboard, circles grid and ChArUco. All chessboard related algorithm expects amount of inner corners as board width and height. Board size of circles grid is just amount of circles by both grid dimentions. ChArUco board size is defined in squares, but detection result is list of inner corners and that's why is smaller by 1 in both dimentions.</dd></dl>
<p>Then again in case of cameras we only take camera images when an input delay time is passed. This is done in order to allow user moving the chessboard around and getting different images. Similar images result in similar equations, and similar equations at the calibration step will form an ill-posed problem, so the calibration will fail. For square images the positions of the corners are only approximate. We may improve this by calling the <a class="el" href="../../dd/d1a/group__imgproc__feature.html#ga354e0d7c86d0d9da75de9b9701a9a87e">cv::cornerSubPix</a> function. (<code>winSize</code> is used to control the side length of the search window. Its default value is 11. <code>winSize</code> may be changed by command line parameter <code>--winSize=&lt;number&gt;</code>.) It will produce better calibration result. After this we add a valid inputs result to the <em>imagePoints</em> vector to collect all of the equations into a single container. Finally, for visualization feedback purposes we will draw the found points on the input image using <a class="el" href="../../d9/d0c/group__calib3d.html#ga93efa9b0aa890de240ca32b11253dd4a">cv::findChessboardCorners</a> function. </p><div class="fragment"><div class="line">        <span class="keywordflow">if</span> (found)                <span class="comment">// If done with success,</span></div>
<div class="line">        {</div>
<div class="line">              <span class="comment">// improve the found corners&#39; coordinate accuracy for chessboard</span></div>
<div class="line">                <span class="keywordflow">if</span>( s.calibrationPattern == Settings::CHESSBOARD)</div>
<div class="line">                {</div>
<div class="line">                    Mat viewGray;</div>
<div class="line">                    cvtColor(view, viewGray, COLOR_BGR2GRAY);</div>
<div class="line">                    cornerSubPix( viewGray, pointBuf, Size(winSize,winSize),</div>
<div class="line">                        Size(-1,-1), TermCriteria( TermCriteria::EPS+TermCriteria::COUNT, 30, 0.0001 ));</div>
<div class="line">                }</div>
<div class="line"> </div>
<div class="line">                <span class="keywordflow">if</span>( mode == CAPTURING &amp;&amp;  <span class="comment">// For camera only take new samples after delay time</span></div>
<div class="line">                    (!s.inputCapture.isOpened() || clock() - prevTimestamp &gt; s.delay*1e-3*CLOCKS_PER_SEC) )</div>
<div class="line">                {</div>
<div class="line">                    imagePoints.push_back(pointBuf);</div>
<div class="line">                    prevTimestamp = clock();</div>
<div class="line">                    blinkOutput = s.inputCapture.isOpened();</div>
<div class="line">                }</div>
<div class="line"> </div>
<div class="line">                <span class="comment">// Draw the corners.</span></div>
<div class="line">                <span class="keywordflow">if</span>(s.calibrationPattern == Settings::CHARUCOBOARD)</div>
<div class="line">                    drawChessboardCorners( view, <a class="code hl_class" href="../../d6/d50/classcv_1_1Size__.html">cv::Size</a>(s.boardSize.width-1, s.boardSize.height-1), Mat(pointBuf), found );</div>
<div class="line">                <span class="keywordflow">else</span></div>
<div class="line">                    drawChessboardCorners( view, s.boardSize, Mat(pointBuf), found );</div>
<div class="line">        }</div>
<div class="ttc" id="aclasscv_1_1Size___html"><div class="ttname"><a href="../../d6/d50/classcv_1_1Size__.html">cv::Size_</a></div><div class="ttdoc">Template class for specifying the size of an image or rectangle.</div><div class="ttdef"><b>Definition</b> types.hpp:335</div></div>
</div><!-- fragment --></li>
<li><p class="startli"><b>Show state and result to the user, plus command line control of the application</b></p>
<p class="startli">This part shows text output on the image. </p><div class="fragment"><div class="line">        <span class="keywordtype">string</span> msg = (mode == CAPTURING) ? <span class="stringliteral">&quot;100/100&quot;</span> :</div>
<div class="line">                      mode == CALIBRATED ? <span class="stringliteral">&quot;Calibrated&quot;</span> : <span class="stringliteral">&quot;Press &#39;g&#39; to start&quot;</span>;</div>
<div class="line">        <span class="keywordtype">int</span> baseLine = 0;</div>
<div class="line">        Size textSize = getTextSize(msg, 1, 1, 1, &amp;baseLine);</div>
<div class="line">        Point textOrigin(view.cols - 2*textSize.width - 10, view.rows - 2*baseLine - 10);</div>
<div class="line"> </div>
<div class="line">        <span class="keywordflow">if</span>( mode == CAPTURING )</div>
<div class="line">        {</div>
<div class="line">            <span class="keywordflow">if</span>(s.showUndistorted)</div>
<div class="line">                msg = <a class="code hl_function" href="../../db/de0/group__core__utils.html#ga0cccdb2f73859309b0611cf70b1b9409">cv::format</a>( <span class="stringliteral">&quot;%d/%d Undist&quot;</span>, (<span class="keywordtype">int</span>)imagePoints.size(), s.nrFrames );</div>
<div class="line">            <span class="keywordflow">else</span></div>
<div class="line">                msg = <a class="code hl_function" href="../../db/de0/group__core__utils.html#ga0cccdb2f73859309b0611cf70b1b9409">cv::format</a>( <span class="stringliteral">&quot;%d/%d&quot;</span>, (<span class="keywordtype">int</span>)imagePoints.size(), s.nrFrames );</div>
<div class="line">        }</div>
<div class="line"> </div>
<div class="line">        putText( view, msg, textOrigin, 1, 1, mode == CALIBRATED ?  GREEN : RED);</div>
<div class="line"> </div>
<div class="line">        <span class="keywordflow">if</span>( blinkOutput )</div>
<div class="line">            bitwise_not(view, view);</div>
<div class="ttc" id="agroup__core__utils_html_ga0cccdb2f73859309b0611cf70b1b9409"><div class="ttname"><a href="../../db/de0/group__core__utils.html#ga0cccdb2f73859309b0611cf70b1b9409">cv::format</a></div><div class="ttdeci">String format(const char *fmt,...)</div><div class="ttdoc">Returns a text string formatted using the printf-like expression.</div></div>
</div><!-- fragment --><p> If we ran calibration and got camera's matrix with the distortion coefficients we may want to correct the image using <a class="el" href="../../d9/d0c/group__calib3d.html#ga69f2545a8b62a6b0fc2ee060dc30559d">cv::undistort</a> function: </p><div class="fragment"><div class="line">        <span class="keywordflow">if</span>( mode == CALIBRATED &amp;&amp; s.showUndistorted )</div>
<div class="line">        {</div>
<div class="line">            Mat temp = view.clone();</div>
<div class="line">            <span class="keywordflow">if</span> (s.useFisheye)</div>
<div class="line">            {</div>
<div class="line">                Mat newCamMat;</div>
<div class="line">                fisheye::estimateNewCameraMatrixForUndistortRectify(cameraMatrix, distCoeffs, imageSize,</div>
<div class="line">                                                                    Matx33d::eye(), newCamMat, 1);</div>
<div class="line">                <a class="code hl_function" href="../../db/d58/group__calib3d__fisheye.html#ga167df4b00a6fd55287ba829fbf9913b9">cv::fisheye::undistortImage</a>(temp, view, cameraMatrix, distCoeffs, newCamMat);</div>
<div class="line">            }</div>
<div class="line">            <span class="keywordflow">else</span></div>
<div class="line">              undistort(temp, view, cameraMatrix, distCoeffs);</div>
<div class="line">        }</div>
<div class="ttc" id="agroup__calib3d__fisheye_html_ga167df4b00a6fd55287ba829fbf9913b9"><div class="ttname"><a href="../../db/d58/group__calib3d__fisheye.html#ga167df4b00a6fd55287ba829fbf9913b9">cv::fisheye::undistortImage</a></div><div class="ttdeci">void undistortImage(InputArray distorted, OutputArray undistorted, InputArray K, InputArray D, InputArray Knew=cv::noArray(), const Size &amp;new_size=Size())</div><div class="ttdoc">Transforms an image to compensate for fisheye lens distortion.</div></div>
</div><!-- fragment --><p> Then we show the image and wait for an input key and if this is <em>u</em> we toggle the distortion removal, if it is <em>g</em> we start again the detection process, and finally for the <em>ESC</em> key we quit the application: </p><div class="fragment"><div class="line">        imshow(<span class="stringliteral">&quot;Image View&quot;</span>, view);</div>
<div class="line">        <span class="keywordtype">char</span> key = (char)waitKey(s.inputCapture.isOpened() ? 50 : s.delay);</div>
<div class="line"> </div>
<div class="line">        <span class="keywordflow">if</span>( key  == ESC_KEY )</div>
<div class="line">            <span class="keywordflow">break</span>;</div>
<div class="line"> </div>
<div class="line">        <span class="keywordflow">if</span>( key == <span class="charliteral">&#39;u&#39;</span> &amp;&amp; mode == CALIBRATED )</div>
<div class="line">           s.showUndistorted = !s.showUndistorted;</div>
<div class="line"> </div>
<div class="line">        <span class="keywordflow">if</span>( s.inputCapture.isOpened() &amp;&amp; key == <span class="charliteral">&#39;g&#39;</span> )</div>
<div class="line">        {</div>
<div class="line">            mode = CAPTURING;</div>
<div class="line">            imagePoints.clear();</div>
<div class="line">        }</div>
</div><!-- fragment --></li>
<li><p class="startli"><b>Show the distortion removal for the images too</b></p>
<p class="startli">When you work with an image list it is not possible to remove the distortion inside the loop. Therefore, you must do this after the loop. Taking advantage of this now I'll expand the <a class="el" href="../../d9/d0c/group__calib3d.html#ga69f2545a8b62a6b0fc2ee060dc30559d">cv::undistort</a> function, which is in fact first calls <a class="el" href="../../d9/d0c/group__calib3d.html#ga7dfb72c9cf9780a347fbe3d1c47e5d5a">cv::initUndistortRectifyMap</a> to find transformation matrices and then performs transformation using <a class="el" href="../../da/d54/group__imgproc__transform.html#gab75ef31ce5cdfb5c44b6da5f3b908ea4">cv::remap</a> function. Because, after successful calibration map calculation needs to be done only once, by using this expanded form you may speed up your application: </p><div class="fragment"><div class="line">    <span class="keywordflow">if</span>( s.inputType == Settings::IMAGE_LIST &amp;&amp; s.showUndistorted &amp;&amp; !cameraMatrix.empty())</div>
<div class="line">    {</div>
<div class="line">        Mat view, rview, map1, map2;</div>
<div class="line"> </div>
<div class="line">        <span class="keywordflow">if</span> (s.useFisheye)</div>
<div class="line">        {</div>
<div class="line">            Mat newCamMat;</div>
<div class="line">            fisheye::estimateNewCameraMatrixForUndistortRectify(cameraMatrix, distCoeffs, imageSize,</div>
<div class="line">                                                                Matx33d::eye(), newCamMat, 1);</div>
<div class="line">            fisheye::initUndistortRectifyMap(cameraMatrix, distCoeffs, Matx33d::eye(), newCamMat, imageSize,</div>
<div class="line">                                             <a class="code hl_define" href="../../d1/d1b/group__core__hal__interface.html#ga50ed0965d0ae7fcd8ee04ec170551ce1">CV_16SC2</a>, map1, map2);</div>
<div class="line">        }</div>
<div class="line">        <span class="keywordflow">else</span></div>
<div class="line">        {</div>
<div class="line">            initUndistortRectifyMap(</div>
<div class="line">                cameraMatrix, distCoeffs, Mat(),</div>
<div class="line">                getOptimalNewCameraMatrix(cameraMatrix, distCoeffs, imageSize, 1, imageSize, 0), imageSize,</div>
<div class="line">                <a class="code hl_define" href="../../d1/d1b/group__core__hal__interface.html#ga50ed0965d0ae7fcd8ee04ec170551ce1">CV_16SC2</a>, map1, map2);</div>
<div class="line">        }</div>
<div class="line"> </div>
<div class="line">        <span class="keywordflow">for</span>(<span class="keywordtype">size_t</span> i = 0; i &lt; s.imageList.size(); i++ )</div>
<div class="line">        {</div>
<div class="line">            view = imread(s.imageList[i], IMREAD_COLOR);</div>
<div class="line">            <span class="keywordflow">if</span>(view.empty())</div>
<div class="line">                <span class="keywordflow">continue</span>;</div>
<div class="line">            remap(view, rview, map1, map2, INTER_LINEAR);</div>
<div class="line">            imshow(<span class="stringliteral">&quot;Image View&quot;</span>, rview);</div>
<div class="line">            <span class="keywordtype">char</span> c = (char)waitKey();</div>
<div class="line">            <span class="keywordflow">if</span>( c  == ESC_KEY || c == <span class="charliteral">&#39;q&#39;</span> || c == <span class="charliteral">&#39;Q&#39;</span> )</div>
<div class="line">                <span class="keywordflow">break</span>;</div>
<div class="line">        }</div>
<div class="line">    }</div>
<div class="ttc" id="agroup__core__hal__interface_html_ga50ed0965d0ae7fcd8ee04ec170551ce1"><div class="ttname"><a href="../../d1/d1b/group__core__hal__interface.html#ga50ed0965d0ae7fcd8ee04ec170551ce1">CV_16SC2</a></div><div class="ttdeci">#define CV_16SC2</div><div class="ttdef"><b>Definition</b> interface.h:107</div></div>
</div><!-- fragment --></li>
</ol>
<h1><a class="anchor" id="autotoc_md233"></a>
The calibration and save</h1>
<p>Because the calibration needs to be done only once per camera, it makes sense to save it after a successful calibration. This way later on you can just load these values into your program. Due to this we first make the calibration, and if it succeeds we save the result into an OpenCV style XML or YAML file, depending on the extension you give in the configuration file.</p>
<p>Therefore in the first function we just split up these two processes. Because we want to save many of the calibration variables we'll create these variables here and pass on both of them to the calibration and saving function. Again, I'll not show the saving part as that has little in common with the calibration. Explore the source file in order to find out how and what: </p><div class="fragment"><div class="line"><span class="keywordtype">bool</span> runCalibrationAndSave(Settings&amp; s, Size imageSize, Mat&amp; cameraMatrix, Mat&amp; distCoeffs,</div>
<div class="line">                           vector&lt;vector&lt;Point2f&gt; &gt; imagePoints, <span class="keywordtype">float</span> grid_width, <span class="keywordtype">bool</span> release_object)</div>
<div class="line">{</div>
<div class="line">    vector&lt;Mat&gt; rvecs, tvecs;</div>
<div class="line">    vector&lt;float&gt; reprojErrs;</div>
<div class="line">    <span class="keywordtype">double</span> totalAvgErr = 0;</div>
<div class="line">    vector&lt;Point3f&gt; newObjPoints;</div>
<div class="line"> </div>
<div class="line">    <span class="keywordtype">bool</span> ok = runCalibration(s, imageSize, cameraMatrix, distCoeffs, imagePoints, rvecs, tvecs, reprojErrs,</div>
<div class="line">                             totalAvgErr, newObjPoints, grid_width, release_object);</div>
<div class="line">    cout &lt;&lt; (ok ? <span class="stringliteral">&quot;Calibration succeeded&quot;</span> : <span class="stringliteral">&quot;Calibration failed&quot;</span>)</div>
<div class="line">         &lt;&lt; <span class="stringliteral">&quot;. avg re projection error = &quot;</span> &lt;&lt; totalAvgErr &lt;&lt; endl;</div>
<div class="line"> </div>
<div class="line">    <span class="keywordflow">if</span> (ok)</div>
<div class="line">        saveCameraParams(s, imageSize, cameraMatrix, distCoeffs, rvecs, tvecs, reprojErrs, imagePoints,</div>
<div class="line">                         totalAvgErr, newObjPoints);</div>
<div class="line">    <span class="keywordflow">return</span> ok;</div>
<div class="line">}</div>
</div><!-- fragment --><p> We do the calibration with the help of the <a class="el" href="../../d9/d0c/group__calib3d.html#ga11eeb16e5a458e1ed382fb27f585b753">cv::calibrateCameraRO</a> function. It has the following parameters:</p>
<ul>
<li>The object points. This is a vector of <em>Point3f</em> vector that for each input image describes how should the pattern look. If we have a planar pattern (like a chessboard) then we can simply set all Z coordinates to zero. This is a collection of the points where these important points are present. Because, we use a single pattern for all the input images we can calculate this just once and multiply it for all the other input views. We calculate the corner points with the <em>calcBoardCornerPositions</em> function as: <div class="fragment"><div class="line"><span class="keyword">static</span> <span class="keywordtype">void</span> calcBoardCornerPositions(Size boardSize, <span class="keywordtype">float</span> squareSize, vector&lt;Point3f&gt;&amp; corners,</div>
<div class="line">                                     Settings::Pattern patternType <span class="comment">/*= Settings::CHESSBOARD*/</span>)</div>
<div class="line">{</div>
<div class="line">    corners.clear();</div>
<div class="line"> </div>
<div class="line">    <span class="keywordflow">switch</span>(patternType)</div>
<div class="line">    {</div>
<div class="line">    <span class="keywordflow">case</span> Settings::CHESSBOARD:</div>
<div class="line">    <span class="keywordflow">case</span> Settings::CIRCLES_GRID:</div>
<div class="line">        <span class="keywordflow">for</span> (<span class="keywordtype">int</span> i = 0; i &lt; boardSize.height; ++i) {</div>
<div class="line">            <span class="keywordflow">for</span> (<span class="keywordtype">int</span> j = 0; j &lt; boardSize.width; ++j) {</div>
<div class="line">                corners.push_back(Point3f(j*squareSize, i*squareSize, 0));</div>
<div class="line">            }</div>
<div class="line">        }</div>
<div class="line">        <span class="keywordflow">break</span>;</div>
<div class="line">    <span class="keywordflow">case</span> Settings::CHARUCOBOARD:</div>
<div class="line">        <span class="keywordflow">for</span> (<span class="keywordtype">int</span> i = 0; i &lt; boardSize.height - 1; ++i) {</div>
<div class="line">            <span class="keywordflow">for</span> (<span class="keywordtype">int</span> j = 0; j &lt; boardSize.width - 1; ++j) {</div>
<div class="line">                corners.push_back(Point3f(j*squareSize, i*squareSize, 0));</div>
<div class="line">            }</div>
<div class="line">        }</div>
<div class="line">        <span class="keywordflow">break</span>;</div>
<div class="line">    <span class="keywordflow">case</span> Settings::ASYMMETRIC_CIRCLES_GRID:</div>
<div class="line">        <span class="keywordflow">for</span> (<span class="keywordtype">int</span> i = 0; i &lt; boardSize.height; i++) {</div>
<div class="line">            <span class="keywordflow">for</span> (<span class="keywordtype">int</span> j = 0; j &lt; boardSize.width; j++) {</div>
<div class="line">                corners.push_back(<a class="code hl_typedef" href="../../dc/d84/group__core__basic.html#ga3d79ceeb4419bccd0308dfdf1cd31435">Point3f</a>((2 * j + i % 2)*squareSize, i*squareSize, 0));</div>
<div class="line">            }</div>
<div class="line">        }</div>
<div class="line">        <span class="keywordflow">break</span>;</div>
<div class="line">    <span class="keywordflow">default</span>:</div>
<div class="line">        <span class="keywordflow">break</span>;</div>
<div class="line">    }</div>
<div class="line">}</div>
<div class="ttc" id="agroup__core__basic_html_ga3d79ceeb4419bccd0308dfdf1cd31435"><div class="ttname"><a href="../../dc/d84/group__core__basic.html#ga3d79ceeb4419bccd0308dfdf1cd31435">cv::Point3f</a></div><div class="ttdeci">Point3_&lt; float &gt; Point3f</div><div class="ttdef"><b>Definition</b> types.hpp:290</div></div>
</div><!-- fragment --> And then multiply it as: <div class="fragment"><div class="line">vector&lt;vector&lt;Point3f&gt; &gt; objectPoints(1);</div>
<div class="line">calcBoardCornerPositions(s.boardSize, s.squareSize, objectPoints[0], s.calibrationPattern);</div>
<div class="line">objectPoints[0][s.boardSize.width - 1].x = objectPoints[0][0].x + grid_width;</div>
<div class="line">newObjPoints = objectPoints[0];</div>
<div class="line"> </div>
<div class="line">objectPoints.resize(imagePoints.size(),objectPoints[0]);</div>
</div><!-- fragment --> <dl class="section note"><dt>Note</dt><dd>If your calibration board is inaccurate, unmeasured, roughly planar targets (Checkerboard patterns on paper using off-the-shelf printers are the most convenient calibration targets and most of them are not accurate enough.), a method from <a class="el" href="../../d0/de3/citelist.html#CITEREF_strobl2011iccv">[254]</a> can be utilized to dramatically improve the accuracies of the estimated camera intrinsic parameters. This new calibration method will be called if command line parameter <code>-d=&lt;number&gt;</code> is provided. In the above code snippet, <code>grid_width</code> is actually the value set by <code>-d=&lt;number&gt;</code>. It's the measured distance between top-left (0, 0, 0) and top-right (s.squareSize*(s.boardSize.width-1), 0, 0) corners of the pattern grid points. It should be measured precisely with rulers or vernier calipers. After calibration, newObjPoints will be updated with refined 3D coordinates of object points.</dd></dl>
</li>
<li>The image points. This is a vector of <em>Point2f</em> vector which for each input image contains coordinates of the important points (corners for chessboard and centers of the circles for the circle pattern). We have already collected this from <a class="el" href="../../d9/d0c/group__calib3d.html#ga93efa9b0aa890de240ca32b11253dd4a">cv::findChessboardCorners</a> or <a class="el" href="../../d9/d0c/group__calib3d.html#ga7f02cd21c8352142890190227628fa80">cv::findCirclesGrid</a> function. We just need to pass it on.</li>
<li>The size of the image acquired from the camera, video file or the images.</li>
<li>The index of the object point to be fixed. We set it to -1 to request standard calibration method. If the new object-releasing method to be used, set it to the index of the top-right corner point of the calibration board grid. See <a class="el" href="../../d9/d0c/group__calib3d.html#ga11eeb16e5a458e1ed382fb27f585b753" title="Finds the camera intrinsic and extrinsic parameters from several views of a calibration pattern.">cv::calibrateCameraRO</a> for detailed explanation. <div class="fragment"><div class="line"><span class="keywordtype">int</span> iFixedPoint = -1;</div>
<div class="line"><span class="keywordflow">if</span> (release_object)</div>
<div class="line">    iFixedPoint = s.boardSize.width - 1;</div>
</div><!-- fragment --></li>
<li>The camera matrix. If we used the fixed aspect ratio option we need to set \(f_x\): <div class="fragment"><div class="line">    cameraMatrix = Mat::eye(3, 3, <a class="code hl_define" href="../../d1/d1b/group__core__hal__interface.html#ga30a562691cc5987bc88eb7bb7a8faf2b">CV_64F</a>);</div>
<div class="line">    <span class="keywordflow">if</span>( !s.useFisheye &amp;&amp; s.flag &amp; CALIB_FIX_ASPECT_RATIO )</div>
<div class="line">        cameraMatrix.at&lt;<span class="keywordtype">double</span>&gt;(0,0) = s.aspectRatio;</div>
<div class="ttc" id="agroup__core__hal__interface_html_ga30a562691cc5987bc88eb7bb7a8faf2b"><div class="ttname"><a href="../../d1/d1b/group__core__hal__interface.html#ga30a562691cc5987bc88eb7bb7a8faf2b">CV_64F</a></div><div class="ttdeci">#define CV_64F</div><div class="ttdef"><b>Definition</b> interface.h:79</div></div>
</div><!-- fragment --></li>
<li>The distortion coefficient matrix. Initialize with zero. <div class="fragment"><div class="line">distCoeffs = Mat::zeros(8, 1, <a class="code hl_define" href="../../d1/d1b/group__core__hal__interface.html#ga30a562691cc5987bc88eb7bb7a8faf2b">CV_64F</a>);</div>
</div><!-- fragment --></li>
<li>For all the views the function will calculate rotation and translation vectors which transform the object points (given in the model coordinate space) to the image points (given in the world coordinate space). The 7-th and 8-th parameters are the output vector of matrices containing in the i-th position the rotation and translation vector for the i-th object point to the i-th image point.</li>
<li>The updated output vector of calibration pattern points. This parameter is ignored with standard calibration method.</li>
<li>The final argument is the flag. You need to specify here options like fix the aspect ratio for the focal length, assume zero tangential distortion or to fix the principal point. Here we use CALIB_USE_LU to get faster calibration speed. <div class="fragment"><div class="line">rms = calibrateCameraRO(objectPoints, imagePoints, imageSize, iFixedPoint,</div>
<div class="line">                        cameraMatrix, distCoeffs, rvecs, tvecs, newObjPoints,</div>
<div class="line">                        s.flag | CALIB_USE_LU);</div>
</div><!-- fragment --></li>
<li>The function returns the average re-projection error. This number gives a good estimation of precision of the found parameters. This should be as close to zero as possible. Given the intrinsic, distortion, rotation and translation matrices we may calculate the error for one view by using the <a class="el" href="../../d9/d0c/group__calib3d.html#ga1019495a2c8d1743ed5cc23fa0daff8c">cv::projectPoints</a> to first transform the object point to image point. Then we calculate the absolute norm between what we got with our transformation and the corner/circle finding algorithm. To find the average error we calculate the arithmetical mean of the errors calculated for all the calibration images. <div class="fragment"><div class="line"><span class="keyword">static</span> <span class="keywordtype">double</span> computeReprojectionErrors( <span class="keyword">const</span> vector&lt;vector&lt;Point3f&gt; &gt;&amp; objectPoints,</div>
<div class="line">                                         <span class="keyword">const</span> vector&lt;vector&lt;Point2f&gt; &gt;&amp; imagePoints,</div>
<div class="line">                                         <span class="keyword">const</span> vector&lt;Mat&gt;&amp; rvecs, <span class="keyword">const</span> vector&lt;Mat&gt;&amp; tvecs,</div>
<div class="line">                                         <span class="keyword">const</span> Mat&amp; cameraMatrix , <span class="keyword">const</span> Mat&amp; distCoeffs,</div>
<div class="line">                                         vector&lt;float&gt;&amp; perViewErrors, <span class="keywordtype">bool</span> fisheye)</div>
<div class="line">{</div>
<div class="line">    vector&lt;Point2f&gt; imagePoints2;</div>
<div class="line">    <span class="keywordtype">size_t</span> totalPoints = 0;</div>
<div class="line">    <span class="keywordtype">double</span> totalErr = 0, err;</div>
<div class="line">    perViewErrors.resize(objectPoints.size());</div>
<div class="line"> </div>
<div class="line">    <span class="keywordflow">for</span>(<span class="keywordtype">size_t</span> i = 0; i &lt; objectPoints.size(); ++i )</div>
<div class="line">    {</div>
<div class="line">        <span class="keywordflow">if</span> (fisheye)</div>
<div class="line">        {</div>
<div class="line">            fisheye::projectPoints(objectPoints[i], imagePoints2, rvecs[i], tvecs[i], cameraMatrix,</div>
<div class="line">                                   distCoeffs);</div>
<div class="line">        }</div>
<div class="line">        <span class="keywordflow">else</span></div>
<div class="line">        {</div>
<div class="line">            projectPoints(objectPoints[i], rvecs[i], tvecs[i], cameraMatrix, distCoeffs, imagePoints2);</div>
<div class="line">        }</div>
<div class="line">        err = norm(imagePoints[i], imagePoints2, NORM_L2);</div>
<div class="line"> </div>
<div class="line">        <span class="keywordtype">size_t</span> n = objectPoints[i].size();</div>
<div class="line">        perViewErrors[i] = (float) std::sqrt(err*err/n);</div>
<div class="line">        totalErr        += err*err;</div>
<div class="line">        totalPoints     += n;</div>
<div class="line">    }</div>
<div class="line"> </div>
<div class="line">    <span class="keywordflow">return</span> std::sqrt(totalErr/totalPoints);</div>
<div class="line">}</div>
</div><!-- fragment --></li>
</ul>
<h1><a class="anchor" id="autotoc_md234"></a>
Results</h1>
<p>Let there be <a href="../../pattern.png" target="_blank">this input chessboard pattern</a> which has a size of 9 X 6. I've used an AXIS IP camera to create a couple of snapshots of the board and saved it into VID5 directory. I've put this inside the <code>images/CameraCalibration</code> folder of my working directory and created the following <code>VID5.XML</code> file that describes which images to use: </p><div class="fragment"><div class="line">&lt;?<span class="keyword">xml</span> <span class="keyword">version</span>=<span class="stringliteral">&quot;1.0&quot;</span>?&gt;</div>
<div class="line">&lt;<span class="keywordtype">opencv_storage</span>&gt;</div>
<div class="line">&lt;<span class="keywordtype">images</span>&gt;</div>
<div class="line"><span class="keyword">images</span>/<span class="keyword">CameraCalibration</span>/<span class="keyword">VID5</span>/<span class="keyword">xx1.jpg</span></div>
<div class="line"><span class="keyword">images</span>/<span class="keyword">CameraCalibration</span>/<span class="keyword">VID5</span>/<span class="keyword">xx2.jpg</span></div>
<div class="line"><span class="keyword">images</span>/<span class="keyword">CameraCalibration</span>/<span class="keyword">VID5</span>/<span class="keyword">xx3.jpg</span></div>
<div class="line"><span class="keyword">images</span>/<span class="keyword">CameraCalibration</span>/<span class="keyword">VID5</span>/<span class="keyword">xx4.jpg</span></div>
<div class="line"><span class="keyword">images</span>/<span class="keyword">CameraCalibration</span>/<span class="keyword">VID5</span>/<span class="keyword">xx5.jpg</span></div>
<div class="line"><span class="keyword">images</span>/<span class="keyword">CameraCalibration</span>/<span class="keyword">VID5</span>/<span class="keyword">xx6.jpg</span></div>
<div class="line"><span class="keyword">images</span>/<span class="keyword">CameraCalibration</span>/<span class="keyword">VID5</span>/<span class="keyword">xx7.jpg</span></div>
<div class="line"><span class="keyword">images</span>/<span class="keyword">CameraCalibration</span>/<span class="keyword">VID5</span>/<span class="keyword">xx8.jpg</span></div>
<div class="line">&lt;/<span class="keywordtype">images</span>&gt;</div>
<div class="line">&lt;/<span class="keywordtype">opencv_storage</span>&gt;</div>
</div><!-- fragment --><p> Then passed <code>images/CameraCalibration/VID5/VID5.XML</code> as an input in the configuration file. Here's a chessboard pattern found during the runtime of the application:</p>
<div class="image">
<img src="../../fileListImage.jpg" alt=""/>
</div>
    <p>After applying the distortion removal we get:</p>
<div class="image">
<img src="../../fileListImageUnDist.jpg" alt=""/>
</div>
    <p>The same works for <a href="../../acircles_pattern.png" target="_blank">this asymmetrical circle pattern</a> by setting the input width to 4 and height to 11. This time I've used a live camera feed by specifying its ID ("1") for the input. Here's, how a detected pattern should look:</p>
<div class="image">
<img src="../../asymetricalPattern.jpg" alt=""/>
</div>
    <p>In both cases in the specified output XML/YAML file you'll find the camera and distortion coefficients matrices: </p><div class="fragment"><div class="line">&lt;<span class="keywordtype">camera_matrix</span> <span class="keyword">type_id</span>=<span class="stringliteral">&quot;opencv-matrix&quot;</span>&gt;</div>
<div class="line">&lt;<span class="keywordtype">rows</span>&gt;3&lt;/<span class="keywordtype">rows</span>&gt;</div>
<div class="line">&lt;<span class="keywordtype">cols</span>&gt;3&lt;/<span class="keywordtype">cols</span>&gt;</div>
<div class="line">&lt;<span class="keywordtype">dt</span>&gt;<span class="keyword">d</span>&lt;/<span class="keywordtype">dt</span>&gt;</div>
<div class="line">&lt;<span class="keywordtype">data</span>&gt;</div>
<div class="line"> 6.5746697944293521<span class="keyword">e</span>+002 0. 3.1950000000000000<span class="keyword">e</span>+002 0.</div>
<div class="line"> 6.5746697944293521<span class="keyword">e</span>+002 2.3950000000000000<span class="keyword">e</span>+002 0. 0. 1.&lt;/<span class="keywordtype">data</span>&gt;&lt;/<span class="keywordtype">camera_matrix</span>&gt;</div>
<div class="line">&lt;<span class="keywordtype">distortion_coefficients</span> <span class="keyword">type_id</span>=<span class="stringliteral">&quot;opencv-matrix&quot;</span>&gt;</div>
<div class="line">&lt;<span class="keywordtype">rows</span>&gt;5&lt;/<span class="keywordtype">rows</span>&gt;</div>
<div class="line">&lt;<span class="keywordtype">cols</span>&gt;1&lt;/<span class="keywordtype">cols</span>&gt;</div>
<div class="line">&lt;<span class="keywordtype">dt</span>&gt;<span class="keyword">d</span>&lt;/<span class="keywordtype">dt</span>&gt;</div>
<div class="line">&lt;<span class="keywordtype">data</span>&gt;</div>
<div class="line"> -4.1802327176423804<span class="keyword">e-001</span> 5.0715244063187526<span class="keyword">e-001</span> 0. 0.</div>
<div class="line"> -5.7843597214487474<span class="keyword">e-001</span>&lt;/<span class="keywordtype">data</span>&gt;&lt;/<span class="keywordtype">distortion_coefficients</span>&gt;</div>
</div><!-- fragment --><p> Add these values as constants to your program, call the <a class="el" href="../../d9/d0c/group__calib3d.html#ga7dfb72c9cf9780a347fbe3d1c47e5d5a">cv::initUndistortRectifyMap</a> and the <a class="el" href="../../da/d54/group__imgproc__transform.html#gab75ef31ce5cdfb5c44b6da5f3b908ea4">cv::remap</a> function to remove distortion and enjoy distortion free inputs for cheap and low quality cameras.</p>
<p>You may observe a runtime instance of this on the <a href="https://www.youtube.com/watch?v=ViPN810E0SU" target="_blank">YouTube here</a>.</p>
<div align='center'><iframe title='Video' width='560' height='349' src='https://www.youtube.com/embed/ViPN810E0SU?rel=0' frameborder='0' align='middle' allowfullscreen></iframe></div> </div></div><!-- contents -->
</div><!-- PageDoc -->
<!-- HTML footer for doxygen 1.8.6-->
<!-- start footer part -->
<hr class="footer"/><address class="footer"><small>
Generated on Sun Jun 2 2024 21:52:13 for OpenCV by &#160;<a href="http://www.doxygen.org/index.html">
<img class="footer" src="../../doxygen.png" alt="doxygen"/>
</a> 1.9.8
</small></address>
<script type="text/javascript">
//<![CDATA[
addTutorialsButtons();
//]]>
</script>
</body>
</html>
